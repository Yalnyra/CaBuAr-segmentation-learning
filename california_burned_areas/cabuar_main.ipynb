{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YQrZFqwW_IUK",
    "outputId": "32624398-b578-41ca-cae7-d7f93ea15b11"
   },
   "outputs": [],
   "source": [
    "# Needs hdf5 to run\n",
    "# Instead of hf datasets lib alternatively use torchgeo \n",
    "# As of 02.11.24 failed to reach geo API, could attempt later\n",
    "%pip install --upgrade torch; torchvision; datasets; h5py; kornia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UsdnMhoO_IUM"
   },
   "outputs": [],
   "source": [
    "# Copyright 2020 The HuggingFace Datasets Authors and the current dataset script contributor.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import datasets\n",
    "# from torchgeo.datasets import CaBuAr\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import functional as F\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from kornia import filters\n",
    "import kornia.augmentation as K\n",
    "\n",
    "\n",
    "# Find for instance the citation on arxiv or on the dataset repo/website\n",
    "_CITATION = \"\"\"\\\n",
    "@article{cabuar,\n",
    "  title={Ca{B}u{A}r: California {B}urned {A}reas dataset for delineation},\n",
    "  author={Rege Cambrin, Daniele and Colomba, Luca and Garza, Paolo},\n",
    "  journal={IEEE Geoscience and Remote Sensing Magazine},\n",
    "  doi={10.1109/MGRS.2023.3292467},\n",
    "  year={2023}\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# You can copy an official description\n",
    "_DESCRIPTION = \"\"\"\\\n",
    "CaBuAr dataset contains images from Sentinel-2 satellites taken before and after a wildfire.\n",
    "The ground truth masks are provided by the California Department of Forestry and Fire Protection and they are mapped on the images.\n",
    "\"\"\"\n",
    "\n",
    "_HOMEPAGE = \"https://huggingface.co/datasets/DarthReca/california_burned_areas\"\n",
    "\n",
    "_LICENSE = \"OPENRAIL\"\n",
    "\n",
    "# Define the root directory for the dataset\n",
    "# Change manually\n",
    "\n",
    "\n",
    "_URLS = {'root': os.curdir,'cache_dir':\"raw\"}\n",
    "_BATCH_SIZE = 16\n",
    "_NUM_WORKERS = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Hugging Face datasets cache directory\n",
    "os.environ['HF_DATASETS_CACHE'] = os.path.join(_URLS['root'],_URLS['cache_dir'])\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "# You can verify by printing (optional)\n",
    "print(f\"HF_DATASETS_CACHE set to: {os.getenv('HF_DATASETS_CACHE')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R9S-LcoE_IUO"
   },
   "outputs": [],
   "source": [
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hb1zyGSq_IUQ"
   },
   "outputs": [],
   "source": [
    "# Load the CaBuAr dataset\n",
    "# Possible splits: 1-5, or chabud\n",
    "# For docs check out \n",
    "# https://huggingface.co/datasets/DarthReca/california_burned_areas\n",
    "# https://torchgeo.readthedocs.io/en/stable/api/datasets.html#cabuar \n",
    "dataset = datasets.load_dataset('DarthReca/california_burned_areas', name='pre-post-fire', split='chabud',trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iiKCr53hP-qL"
   },
   "source": [
    "TODO\n",
    " - Create an class ratio graphic in output mask\n",
    "\n",
    " - **(Optional)** showcase density of classes on mask (how noisy is the data)\n",
    "\n",
    " - Extract spectre distribution on random images\n",
    "\n",
    " - Mean brightness, distribution from center of inputs\n",
    "\n",
    " - how **correlated** are adjacent 2d tensors in pre-fire, post-fire, could dataset be ordered by **time** if needed? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset histogram per frequency layer\n",
    "\n",
    "Important to know whether all layers are equally distributed on mean and variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_histogram(image, bins=256):\n",
    "    \"\"\"\n",
    "    Calculate histogram for each color channel of the image.\n",
    "    \n",
    "    Args:\n",
    "        image (tensor): Image tensor of shape [C, H, W].\n",
    "        bins (int): Number of bins for the histogram.\n",
    "        \n",
    "    Returns:\n",
    "        hist (tensor): Histogram tensor of shape [C, bins].\n",
    "    \"\"\"\n",
    "    hist = torch.zeros((image.size(0), bins))\n",
    "    for c in range(image.size(0)):\n",
    "        hist[c] = torch.histc(image[c], bins=bins, min=0, max=0)\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_color_histogram(batch, bins=256):\n",
    "    \"\"\"\n",
    "    Compute the mean color histogram for a batch of images.\n",
    "    \n",
    "    Args:\n",
    "        batch (tensor): Batch of images of shape [B, C, H, W].\n",
    "        bins (int): Number of bins for the histogram.\n",
    "        \n",
    "    Returns:\n",
    "        mean_hist (tensor): Mean histogram tensor of shape [C, bins].\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size, channels, height, width = batch.size()\n",
    "    histograms = torch.zeros((batch_size, channels, bins))\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        histograms[i] = calculate_histogram(batch[i], bins=bins)\n",
    "    \n",
    "    mean_hist = histograms.mean(dim=0)\n",
    "    return mean_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example batch of images with random values and specified size [batch_size, channels, height, width]\n",
    "batch_size = _BATCH_SIZE\n",
    "channels = 12\n",
    "height = 512\n",
    "width = 512\n",
    "\n",
    "# Generate random images (for demonstration)\n",
    "batch = torch.rand((batch_size, channels, height, width))\n",
    "\n",
    "# Calculate mean color histogram\n",
    "bins = 100\n",
    "mean_hist = mean_color_histogram(batch, bins=bins)\n",
    "\n",
    "# Plot the mean color histogram\n",
    "def plot_histogram(histogram, bins):\n",
    "    # colors = ['r', 'g', 'b']\n",
    "    bin_edges = np.linspace(0, 1, bins + 1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 4))\n",
    "    for c in range(channels):\n",
    "        print(histogram[c].size())\n",
    "        # plt.plot(bin_edges[:-1], histogram[c].numpy(), color=colors[c], alpha=0.5)\n",
    "        plt.plot(bin_edges[:-1], histogram[c].numpy(), alpha=0.5)\n",
    "        #plt.hist(histogram[c].numpy(), bins=bins, alpha=0.5)\n",
    "\n",
    "\n",
    "    plt.xlabel('Pixel Intensity')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Mean Color Histogram')\n",
    "    #plt.legend(['Red', 'Green', 'Blue'])\n",
    "    plt.show()\n",
    "\n",
    "plot_histogram(mean_hist, bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_difference(pre_fire, post_fire):\n",
    "    return post_fire - pre_fire\n",
    "\n",
    "def calculate_ndvi(pre_fire, post_fire):\n",
    "    # Assuming NIR is channel 4 and Red is channel 3 (0-indexed)\n",
    "    pre_nir = pre_fire[:, 4, :, :]\n",
    "    pre_red = pre_fire[:, 3, :, :]\n",
    "    post_nir = post_fire[:, 4, :, :]\n",
    "    post_red = post_fire[:, 3, :, :]\n",
    "    \n",
    "    ndvi_pre = (pre_nir - pre_red) / (pre_nir + pre_red + 1e-6)\n",
    "    ndvi_post = (post_nir - post_red) / (post_nir + post_red + 1e-6)\n",
    "    \n",
    "    return ndvi_pre, ndvi_post\n",
    "\n",
    "def canny(image, lo=0.1, hi=0.2):\n",
    "        return filters.canny(image, low_threshold=lo, high_threshold=hi)\n",
    "\n",
    "def clip_values(image, min_val=0.0, max_val=1.0):\n",
    "    return torch.clamp(image, min=min_val, max=max_val)\n",
    "\n",
    "def unsharp(image):\n",
    "    return filters.unsharp_maskkernel_size(image, (3, 3), sigma=(0.1, 2.0), border_type='refect')\n",
    "\n",
    "def rearrange_channels(pre_fire, post_fire):\n",
    "    # Stack pre-fire and post-fire channels\n",
    "    return torch.cat([pre_fire, post_fire], dim=1)  # Shape: [Batch, 24, H, W]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mozaic = K.RandomJigsaw((6, 6), keepdim=True, p=0.5)\n",
    "normalize = transforms.Normalize(mean=[0.485]*12, std=[0.229]*12)\n",
    "\n",
    "gaussian_blur = transforms.GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 2.0))\n",
    "adjust_contrast = transforms.ColorJitter(contrast=1.5)\n",
    "\n",
    "totensor = transforms.Compose([\n",
    "            transforms.Lambda(np.array), # List converts to numpy array\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    #transforms.Lambda(np.array), # List converts to numpy array\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    mozaic,\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    #transforms.GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 2.0)),\n",
    "    #filters.Laplacian(kernel_size=(3, 3), border_type='reflect'),\n",
    "    # transforms.ToTensor(),\n",
    "    # normalize,\n",
    "])\n",
    "\n",
    "augmentation = transforms.Compose([\n",
    "    transforms.GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 2.0)),\n",
    "    filters.Laplacian(kernel_size=(3, 3), border_type='reflect'),\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_batch(pre_fire, post_fire, mask):\n",
    "    # Clip values\n",
    "    pre_fire = clip_values(pre_fire)\n",
    "    post_fire = clip_values(post_fire)\n",
    "    \n",
    "    # Calculate NDVI or other indices\n",
    "    ndvi_pre, ndvi_post = calculate_ndvi(pre_fire, post_fire)\n",
    "    \n",
    "    # Create difference image\n",
    "    diff_image = compute_difference(pre_fire, post_fire)\n",
    "    \n",
    "    # Concatenate all features\n",
    "    combined_input = torch.cat([pre_fire, post_fire, ndvi_pre.unsqueeze(1), ndvi_post.unsqueeze(1), diff_image.unsqueeze(1)], dim=1)\n",
    "    \n",
    "    # Normalize\n",
    "    combined_input = normalize(combined_input)\n",
    "    \n",
    "    # Augment\n",
    "    combined_input = transform(combined_input)\n",
    "    \n",
    "    # Prepare mask\n",
    "    mask = mask.float()  # or long, depending on loss function\n",
    "    \n",
    "    return combined_input, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature detectors\n",
    "\n",
    "-Important to denoise the data,\n",
    "compute **low** and **high** pass filters, using gaussian and laplacian blur pyramids\n",
    "\n",
    "- quantile tresholding to find peaks\n",
    "\n",
    "- Sebile kernel filter \n",
    "### Detect edges using derivative filters, canny edge detectors and hysteresis tresholding for edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oUe2h8zV_IUR"
   },
   "outputs": [],
   "source": [
    "# Define a custom PyTorch Dataset to apply transformations\n",
    "class CaBuArPyTorchDataset(datasets.GeneratorBasedBuilder):\n",
    "    def __init__(self, hf_dataset, transform=None, augment=None, dropped_channels=None):\n",
    "    # Commented - Alternative def using torchgeo parent class\n",
    "    #def __init__(self, root, transform=None, download = False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hf_dataset: Hugging Face dataset object\n",
    "            transform: Optional transform to be applied on a sample\n",
    "            download: if True, download dataset and store it in the root directory\n",
    "        \"\"\"\n",
    "        # super().__init__(root=root, transforms=transform, download=download)\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.transform = transform\n",
    "        self.augment = augment\n",
    "\n",
    "\n",
    "    def calculate_ndvi(self, pre_fire, post_fire):\n",
    "        # Assuming NIR is channel 5 and Red is channel 4 (0-indexed)\n",
    "        pre_nir = pre_fire[:, 4, :, :]\n",
    "        pre_red = pre_fire[:, 3, :, :]\n",
    "        post_nir = post_fire[:, 4, :, :]\n",
    "        post_red = post_fire[:, 3, :, :]\n",
    "        \n",
    "        ndvi_pre = (pre_nir - pre_red) / (pre_nir + pre_red + 1e-6)\n",
    "        ndvi_post = (post_nir - post_red) / (post_nir + post_red + 1e-6)\n",
    "        \n",
    "        return ndvi_pre.unsqueeze(1), ndvi_post.unsqueeze(1)\n",
    "    \n",
    "\n",
    "    def compute_difference(self, pre_fire, post_fire):\n",
    "        return canny(post_fire) - canny(pre_fire)  # Shape: [B, 12, H, W]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hf_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.hf_dataset[idx]\n",
    "        post_fire = sample['post_fire']  # Shape: (512, 512, 12)\n",
    "        mask = sample['mask']            # Shape: (512, 512, 1)\n",
    "\n",
    "\n",
    "        pre_fire = sample.get('pre_fire', np.zeros_like(post_fire))\n",
    "        \n",
    "        # Convert list files into tensors\n",
    "        pre_fire = totensor(pre_fire).squeeze(0)\n",
    "        post_fire = totensor(post_fire).squeeze(0)\n",
    "        mask = totensor(mask).squeeze(0)\n",
    "\n",
    "        # Calculate NDVI\n",
    "        ndvi_pre, ndvi_post = self.calculate_ndvi(pre_fire, post_fire)\n",
    "        \n",
    "        # Compute difference image\n",
    "        diff_image = self.compute_difference(pre_fire, post_fire)\n",
    "\n",
    "        # Augment layers: edge detection, saturation etc.\n",
    "        if self.augment:\n",
    "            post_fire = self.augment(post_fire)\n",
    "            mask = self.augment(mask)\n",
    "            pre_fire = self.augment(pre_fire)\n",
    "\n",
    "        \n",
    "        # Concatenate pre-fire, post-fire, NDVI, difference\n",
    "        combined_input = torch.cat([pre_fire, post_fire, mask, ndvi_pre, ndvi_post, diff_image], dim=0)  # [24 + 1 + 1 + 1 + 12 = 38 channels]\n",
    "        \n",
    "        # Transform all layers uniformly, to maintain ground truth on validation\n",
    "        combined_input = combined_input.unsqueeze(0)  # Add batch dimension for transforms\n",
    "        if self.transform:\n",
    "            combined_input = self.transform(combined_input)\n",
    "        combined_input = combined_input.squeeze(0)  # Remove batch dimension\n",
    "        \n",
    "        # Prepare mask\n",
    "        mask = mask.float()  # or long, depending on loss function\n",
    "        \n",
    "        if self.transform:\n",
    "            combined_input = self.transform(combined_input)\n",
    "\n",
    "        # Move tensors to device\n",
    "        combined_input = combined_input.to(device)\n",
    "        pre_fire = combined_input[:, :13, :, :]\n",
    "        post_fire = combined_input[:, 13:25, :, :]\n",
    "        mask = combined_input[: 25, :, :]\n",
    "        ndvi_pre = combined_input[:, 26, :, :]\n",
    "        ndvi_post = combined_input[:, 27, :, :]\n",
    "        diff_image = combined_input[:, 27:, :, :]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Prepare the output dictionary\n",
    "        return {\n",
    "                'pre_fire': pre_fire,\n",
    "                'post_fire': post_fire,    # Tensor: [12, 512, 512]\n",
    "                'mask': mask.squeeze(0),    # Tensor: [512, 512]\n",
    "                'ndvi_pre': ndvi_pre,\n",
    "                'ndvi_post': ndvi_post,\n",
    "                'diff_image': diff_image,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nH1E4NAL_IUS"
   },
   "outputs": [],
   "source": [
    "# Instantiate the custom dataset\n",
    "pytorch_dataset = CaBuArPyTorchDataset(\n",
    "    hf_dataset=dataset,\n",
    "    transform=transform,\n",
    "    augment=augmentation,\n",
    ")\n",
    "# pytorch_dataset = CaBuArPyTorchDataset(\n",
    "#     root= _URLS['root'],\n",
    "#     transform=transform,\n",
    "#     download = False  # Set to False if you want to load only post_fire data\n",
    "# )\n",
    "# Create a DataLoader\n",
    "dataloader = DataLoader(\n",
    "    pytorch_dataset,\n",
    "    batch_size=_BATCH_SIZE,       # Adjust batch size as needed\n",
    "    shuffle=False,        # Shuffle for training\n",
    "    num_workers=_NUM_WORKERS,       # Number of subprocesses for data loading\n",
    "    pin_memory=True      # Speed up transfer to GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K4-amxzB_IUS",
    "outputId": "cb2283db-1807-470e-8410-47dc28207b13"
   },
   "outputs": [],
   "source": [
    "# Example: Iterate through the DataLoader\n",
    "for batch_idx, batch in enumerate(dataloader):\n",
    "    post_fire = batch['post_fire']      # Shape: [batch_size, 12, 512, 512]\n",
    "    mask = batch['mask']                # Shape: [batch_size, 512, 512]\n",
    "    pre_fire = batch.get('pre_fire')    # Shape: [batch_size, 12, 512, 512] or None\n",
    "\n",
    "    # Now you can pass `post_fire`, `pre_fire`, and `mask` to your model\n",
    "    # Example:\n",
    "    # outputs = model(post_fire, pre_fire)\n",
    "    # loss = criterion(outputs, mask)\n",
    "\n",
    "    # For demonstration, we'll just print the batch shapes\n",
    "    print(f\"Batch {batch_idx}:\")\n",
    "    print(f\"  post_fire shape: {post_fire.shape}\")\n",
    "    if pre_fire is not None:\n",
    "        print(f\"  pre_fire shape: {pre_fire.shape}\")\n",
    "    print(f\"  mask shape: {mask.shape}\")\n",
    "\n",
    "    # Break after first batch for demonstration\n",
    "    if batch_idx == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot some images\n",
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(X_train[i, :, :, :].numpy().reshape(28, 28), cmap=\"gray_r\")\n",
    "    plt.title(str(y_train[i].item()))\n",
    "j = 0\n",
    "for cls in clss_counts.index:\n",
    "    cls_indices = np.where(y_train.numpy() == cls)[0]\n",
    "    if len(cls_indices) > 0:\n",
    "        plt.subplot(2, len(clss_counts), j + 1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(X_train[cls_indices[0], :, :, :].numpy().reshape(28, 28), cmap=\"gray_r\")\n",
    "        plt.title(str(cls))\n",
    "        j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlexNet + Clusterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import AlexNet\n",
    "from models import KMeansClustering\n",
    "model = AlexNet()\n",
    "print(model)\n",
    "class SemanticSegmentationNetwork(nn.Module):\n",
    "    def __init__(self, num_clusters=6, input_channels=9, num_classes=6):\n",
    "        super(SemanticSegmentationNetwork, self).__init__()\n",
    "        self.feature_extractor = AlexNet(input_channels=input_channels)\n",
    "        self.segmentation_head = KMeansClustering(num_clusters=num_clusters, num_features=256)\n",
    "\n",
    "    def forward(self, pre_fire, post_fire, edge_image):\n",
    "        combined_input = torch.cat((pre_fire, post_fire, edge_image), dim=1)\n",
    "        print(f\"convolution in shape: {combined_input.shape}\")\n",
    "        features = self.feature_extractor(combined_input)\n",
    "        print(f\"convolution out shape: {features.shape}\")\n",
    "        segmentation_mask = self.segmentation_head(features)\n",
    "        return segmentation_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    " - Create an class ratio graphic in output mask\n",
    "\n",
    " - **(Optional)** showcase density of classes on mask (how noisy is the data)\n",
    "\n",
    " - Extract spectre distribution on random images\n",
    "\n",
    " - Mean brightness, distribution from center of inputs\n",
    "\n",
    " - how **correlated** are adjacent 2d tensors in pre-fire, post-fire, could dataset be ordered by **time** if needed? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#from models.semantic_segmentation import SemanticSegmentationNetwork\n",
    "\n",
    "# Create dummy data\n",
    "batch_size, height, width = 1, 224, 224\n",
    "pre_fire = torch.randn(batch_size, 3, height, width)\n",
    "post_fire = torch.randn(batch_size, 3, height, width)\n",
    "edge_image = torch.randn(batch_size, 3, height, width)\n",
    "\n",
    "# Initialize the network\n",
    "model = SemanticSegmentationNetwork(num_clusters=6, input_channels=9)\n",
    "\n",
    "# Forward pass\n",
    "segmentation_mask = model(pre_fire, post_fire, edge_image)\n",
    "\n",
    "# Print the output shape\n",
    "print(\"Segmentation Mask Shape:\", segmentation_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JoPHnLen_IUT"
   },
   "outputs": [],
   "source": [
    "# (Optional) Collect accuracy vector after training epochs\n",
    "# Here, it's assumed you have a training loop where you collect accuracy\n",
    "\n",
    "# Example training loop structure\n",
    "def train_model(dataloader, model, criterion, optimizer, num_epochs=10):\n",
    "    loss_vector = []\n",
    "    accuracy_vector = []\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for batch in dataloader:\n",
    "            post_fire = batch['post_fire']\n",
    "            mask = batch['mask']\n",
    "            pre_fire = batch.get('pre_fire')\n",
    "\n",
    "            # Move data to device\n",
    "            post_fire = post_fire.to(device)\n",
    "            mask = mask.to(device)\n",
    "            if pre_fire is not None:\n",
    "                pre_fire = pre_fire.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(post_fire, pre_fire) if pre_fire is not None else model(post_fire)\n",
    "            loss = criterion(outputs, mask)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Statistics\n",
    "            running_loss += loss.item() * post_fire.size(0)\n",
    "            # Assuming outputs are logits; apply argmax to get predictions\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == mask).sum().item()\n",
    "            total += mask.numel()\n",
    "\n",
    "        epoch_loss = running_loss / len(pytorch_dataset)\n",
    "        epoch_acc = 100. * correct / total\n",
    "        loss_vector.append(epoch_loss)\n",
    "        accuracy_vector.append(epoch_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch}/{num_epochs}: Loss={epoch_loss:.4f}, Accuracy={epoch_acc:.2f}%\")\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "    print(\"Accuracy vector:\", accuracy_vector)\n",
    "    return loss_vector, accuracy_vector"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
