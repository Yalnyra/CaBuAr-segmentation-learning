{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YQrZFqwW_IUK",
    "outputId": "32624398-b578-41ca-cae7-d7f93ea15b11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in d:\\python_course\\nn_practice\\collab\\california_burned_areas\\.conda\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: torchvision in d:\\python_course\\nn_practice\\collab\\california_burned_areas\\.conda\\lib\\site-packages (0.20.1)\n",
      "Requirement already satisfied: datasets in d:\\python_course\\nn_practice\\collab\\california_burned_areas\\.conda\\lib\\site-packages (3.0.1)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: h5py in d:\\python_course\\nn_practice\\collab\\california_burned_areas\\.conda\\lib\\site-packages (3.12.1)\n",
      "Requirement already satisfied: filelock in d:\\python_course\\nn_practice\\collab\\california_burned_areas\\.conda\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in d:\\python_course\\nn_practice\\collab\\california_burned_areas\\.conda\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in d:\\python_course\\nn_practice\\collab\\california_burned_areas\\.conda\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in d:\\python_course\\nn_practice\\collab\\california_burned_areas\\.conda\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in d:\\python_course\\nn_practice\\collab\\california_burned_areas\\.conda\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in d:\\python_course\\nn_practice\\collab\\california_burned_areas\\.conda\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\python_course\\nn_practice\\collab\\california_burned_areas\\.conda\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in d:\\python_course\\nn_practice\\collab\\california_burned_areas\\.conda\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in d:\\python_course\\nn_practice\\collab\\california_burned_areas\\.conda\\lib\\site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in d:\\python_course\\nn_practice\\collab\\california_burned_areas\\.conda\\lib\\site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in d:\\python_course\\nn_practice\\collab\\california_burned_areas\\.conda\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in d:\\python_course\\nn_practice\\collab\\california_burned_areas\\.conda\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in d:\\python_course\\nn_practice\\collab\\california_burned_areas\\.conda\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in d:\\python_course\\nn_practice\\collab\\california_burned_areas\\.conda\\lib\\site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in d:\\python_course\\nn_practice\\collab\\california_burned_areas\\.conda\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in d:\\python_course\\nn_practice\\collab\\california_burned_areas\\.conda\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in d:\\python_course\\nn_practice\\collab\\california_burned_areas\\.conda\\lib\\site-packages (from datasets) (3.10.9)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in d:\\python_course\\nn_practice\\collab\\california_burned_areas\\.conda\\lib\\site-packages (from datasets) (0.25.1)\n",
      "Requirement already satisfied: packaging in d:\\python_course\\nn_practice\\collab\\california_burned_areas\\.conda\\lib\\site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\python_course\\nn_practice\\collab\\california_burned_areas\\.conda\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in d:\\python_course\\nn_practice\\collab\\california_burned_areas\\.conda\\lib\\site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\python_course\\nn_practice\\collab\\california_burned_areas\\.conda\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\python_course\\nn_practice\\collab\\california_burned_areas\\.conda\\lib\\site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\python_course\\nn_practice\\collab\\california_burned_areas\\.conda\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\python_course\\nn_practice\\collab\\california_burned_areas\\.conda\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in d:\\python_course\\nn_practice\\collab\\california_burned_areas\\.conda\\lib\\site-packages (from aiohttp->datasets) (1.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\python_course\\nn_practice\\collab\\california_burned_areas\\.conda\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\python_course\\nn_practice\\collab\\california_burned_areas\\.conda\\lib\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\python_course\\nn_practice\\collab\\california_burned_areas\\.conda\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\python_course\\nn_practice\\collab\\california_burned_areas\\.conda\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: colorama in d:\\python_course\\nn_practice\\collab\\california_burned_areas\\.conda\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\python_course\\nn_practice\\collab\\california_burned_areas\\.conda\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\python_course\\nn_practice\\collab\\california_burned_areas\\.conda\\lib\\site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\python_course\\nn_practice\\collab\\california_burned_areas\\.conda\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\python_course\\nn_practice\\collab\\california_burned_areas\\.conda\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\python_course\\nn_practice\\collab\\california_burned_areas\\.conda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
      "Installing collected packages: datasets\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 3.0.1\n",
      "    Uninstalling datasets-3.0.1:\n",
      "      Successfully uninstalled datasets-3.0.1\n",
      "Successfully installed datasets-3.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Needs hdf5 to run\n",
    "# Instead of hf datasets lib alternatively use torchgeo \n",
    "# As of 02.11.24 failed to reach geo API, could attempt later\n",
    "%pip install --upgrade torch; torchvision; datasets; h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UsdnMhoO_IUM"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python_course\\NN_Practice\\collab\\california_burned_areas\\.conda\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Copyright 2020 The HuggingFace Datasets Authors and the current dataset script contributor.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import os\n",
    "from typing import List\n",
    "import torch\n",
    "import datasets\n",
    "# from torchgeo.datasets import CaBuAr\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "# Find for instance the citation on arxiv or on the dataset repo/website\n",
    "_CITATION = \"\"\"\\\n",
    "@article{cabuar,\n",
    "  title={Ca{B}u{A}r: California {B}urned {A}reas dataset for delineation},\n",
    "  author={Rege Cambrin, Daniele and Colomba, Luca and Garza, Paolo},\n",
    "  journal={IEEE Geoscience and Remote Sensing Magazine},\n",
    "  doi={10.1109/MGRS.2023.3292467},\n",
    "  year={2023}\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# You can copy an official description\n",
    "_DESCRIPTION = \"\"\"\\\n",
    "CaBuAr dataset contains images from Sentinel-2 satellites taken before and after a wildfire.\n",
    "The ground truth masks are provided by the California Department of Forestry and Fire Protection and they are mapped on the images.\n",
    "\"\"\"\n",
    "\n",
    "_HOMEPAGE = \"https://huggingface.co/datasets/DarthReca/california_burned_areas\"\n",
    "\n",
    "_LICENSE = \"OPENRAIL\"\n",
    "\n",
    "# Define the root directory for the dataset\n",
    "# Change manually\n",
    "_URLS = {'root': os.curdir,'cache_dir':\"/raw\"}\n",
    "_BATCH_SIZE = 16\n",
    "_NUM_WORKERS = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_DATASETS_CACHE set to: D:\\\\python_course\\\\NN_Practice\\\\collab\\\\california_burned_areas\\\\raw\n"
     ]
    }
   ],
   "source": [
    "# Set the Hugging Face datasets cache directory\n",
    "os.environ['HF_DATASETS_CACHE'] = os.path.join(_URLS['root'],_URLS['cache_dir'])\n",
    "\n",
    "\n",
    "# You can verify by printing (optional)\n",
    "print(f\"HF_DATASETS_CACHE set to: {os.getenv('HF_DATASETS_CACHE')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "R9S-LcoE_IUO"
   },
   "outputs": [],
   "source": [
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Hb1zyGSq_IUQ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python_course\\NN_Practice\\collab\\california_burned_areas\\.conda\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\pc\\.cache\\huggingface\\hub\\datasets--DarthReca--california_burned_areas. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Generating 0 split: 78 examples [00:11,  6.53 examples/s]\n",
      "Generating 1 split: 55 examples [00:09,  6.10 examples/s]\n",
      "Generating 2 split: 69 examples [00:11,  5.77 examples/s]\n",
      "Generating 3 split: 85 examples [00:13,  6.22 examples/s]\n",
      "Generating 4 split: 69 examples [00:10,  6.33 examples/s]\n",
      "Generating chabud split: 68 examples [00:19,  3.54 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Load the CaBuAr dataset\n",
    "# Possible splits: 1-5, or chabud\n",
    "# For docs check out \n",
    "# https://huggingface.co/datasets/DarthReca/california_burned_areas\n",
    "# https://torchgeo.readthedocs.io/en/stable/api/datasets.html#cabuar \n",
    "dataset = datasets.load_dataset('DarthReca/california_burned_areas', name='pre-post-fire', split='chabud',trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "YOhcs4dcJB_i"
   },
   "outputs": [],
   "source": [
    "# Define transformations (modify as needed)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Lambda(np.array), # List converts to numpy array\n",
    "    transforms.ToTensor(),  # Converts numpy arrays to torch tensors\n",
    "    # Add more transforms if necessary, e.g., normalization\n",
    "    # transforms.Normalize(mean=[0.5], std=[0.5]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "oUe2h8zV_IUR"
   },
   "outputs": [],
   "source": [
    "# Define a custom PyTorch Dataset to apply transformations\n",
    "class CaBuArPyTorchDataset(datasets.GeneratorBasedBuilder):\n",
    "    def __init__(self, hf_dataset, transform=None, load_prefire=True):\n",
    "    # Commented - Alternative def using torchgeo parent class\n",
    "    #def __init__(self, root, transform=None, download = False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hf_dataset: Hugging Face dataset object\n",
    "            transform: Optional transform to be applied on a sample\n",
    "            load_prefire: Boolean indicating whether to load pre_fire data\n",
    "            download: if True, download dataset and store it in the root directory\n",
    "        \"\"\"\n",
    "        # super().__init__(root=root, transforms=transform, download=download)\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.transform = transform\n",
    "        self.load_prefire = load_prefire\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hf_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.hf_dataset[idx]\n",
    "        post_fire = sample['post_fire']  # Shape: (512, 512, 12)\n",
    "        mask = sample['mask']            # Shape: (512, 512, 1)\n",
    "\n",
    "        if self.load_prefire:\n",
    "            pre_fire = sample.get('pre_fire', np.zeros_like(post_fire))\n",
    "        else:\n",
    "            pre_fire = None\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            post_fire = self.transform(post_fire)\n",
    "            mask = self.transform(mask)\n",
    "            if pre_fire is not None:\n",
    "                pre_fire = self.transform(pre_fire)\n",
    "\n",
    "        # Move tensors to device\n",
    "        post_fire = post_fire.to(device)\n",
    "        mask = mask.to(device)\n",
    "        if pre_fire is not None:\n",
    "            pre_fire = pre_fire.to(device)\n",
    "\n",
    "        # Prepare the output dictionary\n",
    "        if self.load_prefire:\n",
    "            return {\n",
    "                'post_fire': post_fire,    # Tensor: [12, 512, 512]\n",
    "                'pre_fire': pre_fire,      # Tensor: [12, 512, 512]\n",
    "                'mask': mask.squeeze(0)    # Tensor: [512, 512]\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'post_fire': post_fire,    # Tensor: [12, 512, 512]\n",
    "                'mask': mask.squeeze(0)    # Tensor: [512, 512]\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "nH1E4NAL_IUS"
   },
   "outputs": [],
   "source": [
    "# Instantiate the custom dataset\n",
    "pytorch_dataset = CaBuArPyTorchDataset(\n",
    "    hf_dataset=dataset,\n",
    "    transform=transform,\n",
    "    load_prefire=True  # Set to False if you want to load only post_fire data\n",
    ")\n",
    "# pytorch_dataset = CaBuArPyTorchDataset(\n",
    "#     root= _URLS['root'],\n",
    "#     transform=transform,\n",
    "#     download = False  # Set to False if you want to load only post_fire data\n",
    "# )\n",
    "# Create a DataLoader\n",
    "dataloader = DataLoader(\n",
    "    pytorch_dataset,\n",
    "    batch_size=_BATCH_SIZE,       # Adjust batch size as needed\n",
    "    shuffle=False,        # Shuffle for training\n",
    "    num_workers=_NUM_WORKERS,       # Number of subprocesses for data loading\n",
    "    pin_memory=True      # Speed up transfer to GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K4-amxzB_IUS",
    "outputId": "cb2283db-1807-470e-8410-47dc28207b13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0:\n",
      "  post_fire shape: torch.Size([16, 12, 512, 512])\n",
      "  pre_fire shape: torch.Size([16, 12, 512, 512])\n",
      "  mask shape: torch.Size([16, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "# Example: Iterate through the DataLoader\n",
    "for batch_idx, batch in enumerate(dataloader):\n",
    "    post_fire = batch['post_fire']      # Shape: [batch_size, 12, 512, 512]\n",
    "    mask = batch['mask']                # Shape: [batch_size, 512, 512]\n",
    "    pre_fire = batch.get('pre_fire')    # Shape: [batch_size, 12, 512, 512] or None\n",
    "\n",
    "    # Now you can pass `post_fire`, `pre_fire`, and `mask` to your model\n",
    "    # Example:\n",
    "    # outputs = model(post_fire, pre_fire)\n",
    "    # loss = criterion(outputs, mask)\n",
    "\n",
    "    # For demonstration, we'll just print the batch shapes\n",
    "    print(f\"Batch {batch_idx}:\")\n",
    "    print(f\"  post_fire shape: {post_fire.shape}\")\n",
    "    if pre_fire is not None:\n",
    "        print(f\"  pre_fire shape: {pre_fire.shape}\")\n",
    "    print(f\"  mask shape: {mask.shape}\")\n",
    "\n",
    "    # Break after first batch for demonstration\n",
    "    if batch_idx == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iiKCr53hP-qL"
   },
   "source": [
    "# TODO\n",
    " - Create an class ratio graphic in output mask\n",
    "\n",
    " - **(Optional)** showcase density of classes on mask (how noisy is the data)\n",
    "\n",
    " - Extract spectre distribution on random images\n",
    "\n",
    " - Mean brightness, distribution from center of inputs\n",
    "\n",
    " - how **correlated** are adjacent 2d tensors in pre-fire, post-fire, could dataset be ordered by **time** if needed? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "JoPHnLen_IUT"
   },
   "outputs": [],
   "source": [
    "# (Optional) Collect accuracy vector after training epochs\n",
    "# Here, it's assumed you have a training loop where you collect accuracy\n",
    "\n",
    "# Example training loop structure\n",
    "def train_model(dataloader, model, criterion, optimizer, num_epochs=10):\n",
    "    loss_vector = []\n",
    "    accuracy_vector = []\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for batch in dataloader:\n",
    "            post_fire = batch['post_fire']\n",
    "            mask = batch['mask']\n",
    "            pre_fire = batch.get('pre_fire')\n",
    "\n",
    "            # Move data to device\n",
    "            post_fire = post_fire.to(device)\n",
    "            mask = mask.to(device)\n",
    "            if pre_fire is not None:\n",
    "                pre_fire = pre_fire.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(post_fire, pre_fire) if pre_fire is not None else model(post_fire)\n",
    "            loss = criterion(outputs, mask)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Statistics\n",
    "            running_loss += loss.item() * post_fire.size(0)\n",
    "            # Assuming outputs are logits; apply argmax to get predictions\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == mask).sum().item()\n",
    "            total += mask.numel()\n",
    "\n",
    "        epoch_loss = running_loss / len(pytorch_dataset)\n",
    "        epoch_acc = 100. * correct / total\n",
    "        loss_vector.append(epoch_loss)\n",
    "        accuracy_vector.append(epoch_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch}/{num_epochs}: Loss={epoch_loss:.4f}, Accuracy={epoch_acc:.2f}%\")\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "    print(\"Accuracy vector:\", accuracy_vector)\n",
    "    return loss_vector, accuracy_vector"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
